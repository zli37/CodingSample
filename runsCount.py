#!/usr/bin/env python
#
#   This code is for the homework
#   The code gives the number of increasing runs of
#   the length of k (user defined). Variable k
#   can be modified by users. The default value is 7
#   To run the code using spark-submit is as follows:
#   spark-submit runscount.py 2>log >result
#   The log file has some systematic info and the
#   result file should have the number of counts.

from pyspark import SparkConf, SparkContext
import subprocess

# Set up yarn and initialize sc
conf = SparkConf()
conf.setMaster("yarn-client")
conf.setAppName("My Increading Runs Counting Application")
conf.set("spark.executor.memory","1g")
sc = SparkContext(conf = conf)

# Initialize dataRDD
rawRDD = sc.textFile("file_name")   # file:/// to locate local file
dataRDD = rawRDD.flatMap(lambda line: line.split())
numRDD = dataRDD.map(lambda e: float(e))

# Length of the increasing runs
# This is set to be 3 initially and can be changed
# Length can not be 0

####################################
k = 3
####################################

# Function window() returns a new RDD consisting of
# all possible segments with length of k
def window(baseRDD):

    # Local function destination(i)
    # Given i, the index of an element in some RDD,
    # generate a window of length k with i at its beginning.
    def destination(i):
        right = [i+j for j in range(1,k)]
        r = [j for j in [i] + right]
        return tuple(r)

    # Local function spread(v)
    # Given v which is a pair (value, index), generate
    # a tuple with pairs (t,v) for each element
    # in destinations(index)
    def spread(v):
        assert len(v) == 2
        value, index = v
        out = [(t,v) for t in destination(index)]
        return tuple(out)

    # Local function segmented(v)
    # Map collected neighboring values into a tuple, 
    # where the values are like those generated by
    # the spread(v) function
    def segmented(v):
        ind, valist = v
        sortvalist = sorted(valist,key=lambda x: x[1])
        w = [a for a,b in sortvalist]
        return (ind,w)

    # Now we code the window(baseRDD) function
    indRDD = baseRDD.zipWithIndex()
    spreadRDD = indRDD.flatMap(lambda v: spread(v))
    rawWinRDD = spreadRDD.groupByKey()
    winDowRDD = rawWinRDD.map(lambda v: segmented(v))
    return winDowRDD

# Now run the counting 
# First window numRDD
runRDD = window(numRDD)
# Throw out bad cases in terms of length
runRDD = runRDD.filter(lambda s:len(s[1]) == k)
# A helper function to throw out the ascending but not
# increasing
def inc(runs):
    tmpSorted = sorted(runs)
    tmp = 0.0
    for i,j in enumerate(tmpSorted):
        if i == 0:
            tmp = j
        else:
            if j != tmp:
                tmp = j
            else:
                return False
    return True

# Throw out the bad cases in terms of ascending not increasing
runRDD = runRDD.filter(lambda s: inc(s[1]))
# Only keep the increasing runs
runRDD = runRDD.filter(lambda s:sorted(s[1]) == list(s[1]))
# Finally the count is the number of increasing runs
counts = runRDD.count()
print "The number of increasing runs of length", k, "is"
print counts
